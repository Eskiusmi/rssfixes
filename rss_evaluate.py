
import json, os, re, time, requests, feedparser, difflib, csv
from datetime import datetime, timedelta
from dateutil import parser as date_parser

GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_API_BASE = "https://api.groq.com/openai/v1"
GROQ_MODEL = "llama3-8b-8192"
HEADERS = {"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type": "application/json"}

HOURS_WINDOW = 24
DB_FILE = "rss_db.json"
WHITELIST_FILE = "university_whitelist.csv"

DIMENSION_ORDER = ["æ•™è‚²å…³è”åº¦", "çƒ­åº¦ä¸å†²å‡»åŠ›", "æ–°é¢–æ€§ä¸è§†è§‰æ€§", "å»¶å±•æ€§ä¸æ·±åº¦", "å—ä¼—åŒ¹é…åº¦"]
WEIGHTS = [0.30, 0.25, 0.20, 0.15, 0.10]

def load_whitelist():
    if not os.path.exists(WHITELIST_FILE):
        print("âš ï¸ æœªæ‰¾åˆ°åå•æ–‡ä»¶ university_whitelist.csvï¼Œé»˜è®¤ä¸è¿‡æ»¤")
        return None
    with open(WHITELIST_FILE, encoding="utf-8") as f:
        return set(row["university"].strip() for row in csv.DictReader(f))

def make_prompt(title, summary):
    return f"""ä½ æ˜¯æ•™è‚²é¢†åŸŸå†…å®¹åˆ›ä½œè€…çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·å¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œå¤šç»´åº¦è¯„åˆ†å’Œè§£é‡Šã€‚
è¦æ±‚ï¼š
- å¯¹æ¯ä¸ªç»´åº¦æ‰“ 1~10 åˆ†ï¼Œå¹¶å†™ä¸€å¥ç†ç”±è¯´æ˜ã€‚
- æœ€åç”Ÿæˆè‹±æ–‡æ‘˜è¦ï¼Œå¹¶è®¡ç®—åŠ æƒå¹³å‡çš„ç»¼åˆè¯„åˆ†ã€‚
ç»´åº¦ï¼š
1. æ•™è‚²å…³è”åº¦
2. çƒ­åº¦ä¸å†²å‡»åŠ›
3. æ–°é¢–æ€§ä¸è§†è§‰æ€§
4. å»¶å±•æ€§ä¸æ·±åº¦
5. å—ä¼—åŒ¹é…åº¦
è¯·è¿”å› JSONï¼Œå¦‚ä¸‹æ ¼å¼ï¼š
{{
  "ç»´åº¦è¯„åˆ†": {{
    "æ•™è‚²å…³è”åº¦": {{"åˆ†æ•°": 8, "è¯´æ˜": "ä¸æ•™è‚²æ”¿ç­–å¼ºç›¸å…³"}},
    ...
  }},
  "summary": "...",
  "score": 8.2
}}
æ–°é—»æ ‡é¢˜: {title}
æ–°é—»æ‘˜è¦: {summary}
"""

def evaluate(title, summary):
    payload = {
        "model": GROQ_MODEL,
        "messages": [{"role": "user", "content": make_prompt(title, summary)}],
        "temperature": 0.3
    }
    try:
        r = requests.post(f"{GROQ_API_BASE}/chat/completions", headers=HEADERS, json=payload, timeout=60)
        r.raise_for_status()
        txt = r.json()["choices"][0]["message"]["content"]
        m = re.search(r"\{.*\}", txt, re.S)
        data = json.loads(m.group(0)) if m else {}
        dims = data.get("ç»´åº¦è¯„åˆ†", {})
        edu_score = dims.get("æ•™è‚²å…³è”åº¦", {}).get("åˆ†æ•°", 0)
        if edu_score < 6:
            print(f"âš ï¸ æ•™è‚²å…³è”åº¦è¿‡ä½ï¼ˆ{edu_score}ï¼‰ï¼Œå¿½ç•¥")
            return None
        score = sum((dims[d]["åˆ†æ•°"] if isinstance(dims.get(d), dict) else 0) * w for d, w in zip(DIMENSION_ORDER, WEIGHTS))
        return {"score": round(score, 2), "summary": data.get("summary", ""), "dimensions": dims}
    except Exception as e:
        print("âŒ Groq error:", e)
        return None

def parse_datetime_safe(raw):
    try:
        dt = date_parser.parse(raw)
        if dt.year < 2020:
            return datetime.utcnow().replace(tzinfo=None)
        return dt.replace(tzinfo=None)
    except:
        return datetime.utcnow().replace(tzinfo=None)

def is_similar(t1, t2, threshold=0.4):
    return difflib.SequenceMatcher(None, t1, t2).ratio() >= threshold

def llm_is_duplicate(t1, s1, t2, s2):
    prompt = f"""
åˆ¤æ–­ä»¥ä¸‹ä¸¤ä¸ªæ–°é—»æ˜¯å¦æè¿°çš„æ˜¯åŒä¸€äº‹ä»¶ï¼Œä¸è€ƒè™‘è¯­è¨€é£æ ¼æˆ–è¡¨è¾¾æ–¹å¼ï¼Œåªçœ‹æ˜¯å¦æŠ¥é“çš„æ˜¯ç›¸åŒäº‹ä»¶ã€‚
æ ‡é¢˜1: {t1}
æ‘˜è¦1: {s1}
æ ‡é¢˜2: {t2}
æ‘˜è¦2: {s2}
è¿”å›æ ¼å¼:
{{"same_event": true/false}}
"""
    try:
        payload = {
            "model": GROQ_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.2
        }
        res = requests.post(f"{GROQ_API_BASE}/chat/completions", headers=HEADERS, json=payload, timeout=30)
        res.raise_for_status()
        txt = res.json()["choices"][0]["message"]["content"]
        match = re.search(r"\{.*\}", txt, re.S)
        result = json.loads(match.group(0)) if match else {}
        time.sleep(2.1)
        return result.get("same_event", False)
    except Exception as e:
        print("âš ï¸ LLM å»é‡å¤±è´¥:", e)
        return False

def deduplicate(items):
    unique = []
    for item in items:
        is_dup = False
        for u in unique:
            if is_similar(item["title"], u["title"], threshold=0.4):
                if llm_is_duplicate(item["title"], item["summary"], u["title"], u["summary"]):
                    if item["published"] < u["published"]:
                        unique.remove(u)
                        unique.append(item)
                    is_dup = True
                    break
        if not is_dup:
            unique.append(item)
    return unique

def fetch_rss(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        print("ğŸ“¥ æŠ“å–:", url)
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        parsed = feedparser.parse(resp.content)
        print(f"âœ… è¿”å›æ¡ç›®æ•°ï¼š{len(parsed.entries)}")
        return parsed
    except Exception as e:
        print("âŒ æŠ“å–å¤±è´¥:", e)
        return feedparser.parse("")

def load_db():
    if os.path.exists(DB_FILE):
        return json.load(open(DB_FILE, encoding="utf-8"))
    return []

def save_db(data):
    with open(DB_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def collect(feeds="feeds.json"):
    now = datetime.utcnow().replace(tzinfo=None)
    whitelist = load_whitelist()
    items = []
    for f in json.load(open(feeds, encoding="utf-8")):
        rss = f.get("rss")
        if not rss:
            continue
        parsed = fetch_rss(rss)
        for e in parsed.entries:
            title = e.get("title", "").strip()
            link = e.get("link", "").strip()
            summary = e.get("summary", "") or e.get("content", [{}])[0].get("value", "")
            published_raw = e.get("published", "") or e.get("updated", "") or e.get("created", "")
            pub_time = parse_datetime_safe(published_raw)
            text = f"{title} {summary}"
            if whitelist and not any(uni.lower() in text.lower() for uni in whitelist):
                continue
            if now - pub_time > timedelta(hours=HOURS_WINDOW):
                continue
            items.append({
                "university": f.get("university", "Google Alert"),
                "title": title,
                "link": link,
                "summary": summary,
                "published": pub_time.isoformat()
            })
    print(f"âœ… æŠ“å–å®Œæˆï¼Œå…±æ”¶é›†æ–°é—»ï¼š{len(items)}")
    return items

def main():
    today = deduplicate(collect())
    print(f"ğŸ§  å»é‡åå‰©ä½™æ–°é—»ï¼š{len(today)}")
    results = []
    for item in today:
        print("â†’ è¯„åˆ†ä¸­:", item["title"][:60])
        result = evaluate(item["title"], item["summary"])
        time.sleep(2.1)
        if result:
            item.update(result)
            results.append(item)
    old = load_db()
    combined = deduplicate(results + old)
    now = datetime.utcnow().replace(tzinfo=None)
    fresh = []
    for i in combined:
        try:
            dt = parse_datetime_safe(i["published"])
            if now - dt < timedelta(hours=HOURS_WINDOW):
                fresh.append(i)
        except:
            continue
    save_db(fresh)
    json.dump(fresh, open("evaluated_results.json", "w", encoding="utf-8"), ensure_ascii=False, indent=2)
    print(f"ğŸ“¦ å†™å…¥ rss_db.json å…± {len(fresh)} æ¡æ–°é—»")
    print("âœ… å…¨éƒ¨å®Œæˆ")

if __name__ == "__main__":
    main()
